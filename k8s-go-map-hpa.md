# K8s 明明设置了内存阈值，为什么服务只会扩容，不会缩容？

线上环境又出问题啦😒,十几个节点上的 Pod 空转着，每小时都在烧钱呀，事情是这样的，上周我上线了一个 Go 服务,并且配置了 HPA双指标扩缩容**：

- **CPU超标 → 扩容** ✅
- **内存超标 → 扩容** ✅
- **指标下降 → 缩容** ❌

为此我提前计算过一个Pod的大概情况：

- 一个 Pod 预计处理 **10 万连接**
- 每个连接在内存里有一份状态(这个是服务有特殊业务需求)
- 状态存放在一个 map 里

逻辑模型大致是：

```go
type ConnState struct {
    ConnID  string
	  ......
}
map[string]ConnState
```

我大概估算过单连接的内存成本，也给 HPA 设了明确阈值(我当时非常确信：**这波不可能是我代码的问题**)：

> **一旦指标超过这个内存值，就扩容**

上到测试环境进行压里测试，一切都按预期运行：流量飙升，内存上涨，Pod数量瞬间拉起。
正当我准备庆功时，诡异的事情发生了——

**压测结束后，Pod数量卡在高位纹丝不动！**

连接数从10万降到1万，CPU几乎归零，但内存指标并没有按预期的下降到常规状态。
HPA眼睁睁看着高内存占用，坚决不肯缩容。
我排查了所有可能性：HPA配置、Metrics Server、采集延迟……
直到我把目光投向了自己写的Go代码。

---

## 20行代码，揭开K8s缩容谜题

让我写个最简单的实验重现这个问题：

```go
package main

import (
    "fmt"
    "runtime"
)

func printAlloc() {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    fmt.Printf("%d MB\n", m.Alloc/(1024*1024))
}

func main() {
    n := 1_000_000
    m := make(map[int][128]byte)
    printAlloc() // 初始
		// 模拟百万连接
    for i := 0; i < n; i++ {
        m[i] = [128]byte{} // 我用unsafe.size 测过struct size <128
    }
    printAlloc() // 模拟高峰连接数
		
  	// 模拟连接全部断开
    for i := 0; i < n; i++ {
        delete(m, i)
    }
    runtime.GC()
    printAlloc()      
    runtime.KeepAlive(m)
}
```
## 运行结果令人震惊：

```
初始：0 MB
高峰：461 MB  
回落后：293 MB  // 竟然还占着大半内存！
```

---

## 真相：Go map的“内存陷阱”（slice 也一样）

问题出在Go map的底层设计上,我忽略了map没有缩容策略：

``````tex
delete(map, key)
// delete 只是“清空内容”，不是“拆掉容器”
``````

这就好比：

- **高峰期**：酒店住满客人（map存满数据）
- **淡季**：客人全部退房（delete所有key）
- **但酒店**：仍保留所有房间结构，不拆楼！

**所以很多所谓的“内存泄漏”，本质上只是 Go 没帮你缩容。**

HPA看到的正是这个“保留着空房间的酒店”——内存占用居高不下，自然不敢缩容。



为此必须自己实现map的替换，让原来的map 不再引用，让GC回收掉它。

---

## 一个字节引发的“血案”

前面我留了个坑，强调了一下我的struct size <128：
把 `[128]byte` 改成 `[129]byte`，内存表现会天差地别。

```go
[128]byte
```

改成：

```go
[129]byte
```

内存表现会明显不同。

为什么只差 1 个字节，却能影响 Go map 内存布局，  我准备明天再讲😁

这背后涉及到：
	•	Go map bucket 的布局
	•	value 是否内联存储
	•	什么时候会变成指针逃逸

如果你线上服务也遇到过「内存下不来，HPA 不敢缩」的问题，这不一定是内存泄漏，
下一篇你一定要看。